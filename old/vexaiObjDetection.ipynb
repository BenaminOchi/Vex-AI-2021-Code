{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vexaiObjDetection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcDdt-klRWzl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20c68929-501b-4fab-fe23-5a5e8ba45d77"
      },
      "source": [
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import pycocotools\n",
        "import os\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from PIL import Image, ImageDraw\n",
        "import pandas as pd\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOdOpHkTr0tj",
        "outputId": "51d6a56b-9dc3-4d98-a191-745fe54fc040"
      },
      "source": [
        "!ls /content/gdrive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MyDrive  Shareddrives\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnH8PHK8diBj"
      },
      "source": [
        "def xml_to_csv(path):\n",
        "    xml_list = []\n",
        "    for xml_file in glob.glob(path + '/*.xml'):\n",
        "        tree = ET.parse(xml_file)\n",
        "        root = tree.getroot()\n",
        "        for member in root.findall('object'):\n",
        "            value = (root.find('filename').text,\n",
        "                     int(root.find('size')[0].text),\n",
        "                     int(root.find('size')[1].text),\n",
        "                     member[0].text,\n",
        "                     int(member[4][0].text),\n",
        "                     int(member[4][1].text),\n",
        "                     int(member[4][2].text),\n",
        "                     int(member[4][3].text)\n",
        "                     )\n",
        "            xml_list.append(value)\n",
        "    column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
        "    xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
        "    return xml_df\n",
        "\n",
        "def convert():\n",
        "    image_path = os.path.join(os.getcwd(), 'annotations')\n",
        "    image_path = '/content/raccoon_dataset/annotations'\n",
        "    xml_df = xml_to_csv(image_path)\n",
        "    xml_df.to_csv('vexai_labels.csv', index=None)\n",
        "    print(xml_df)\n",
        "    print('Successfully converted xml to csv.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4aaytd0_Ugy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2493a14-cef5-440d-f2f2-9358669cd555"
      },
      "source": [
        "#get dataset and convert xml to csv\n",
        "#because it is a private repo we have to use an access token\n",
        "%cd /content/raccoon_dataset/annotations\n",
        "#%rm -rf Vex-AI-2021-Code/\n",
        "#%rm -rf raccoon_dataset/\n",
        "%pwd\n",
        "#!git clone https://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@github.com/BenaminOchi/Vex-AI-2021-Code.git\n",
        "!git clone https://github.com/datitran/raccoon_dataset.git\n",
        "convert()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/raccoon_dataset/annotations\n",
            "Cloning into 'raccoon_dataset'...\n",
            "remote: Enumerating objects: 652, done.\u001b[K\n",
            "remote: Total 652 (delta 0), reused 0 (delta 0), pack-reused 652\u001b[K\n",
            "Receiving objects: 100% (652/652), 48.01 MiB | 35.21 MiB/s, done.\n",
            "Resolving deltas: 100% (415/415), done.\n",
            "            filename  width  height    class  xmin  ymin  xmax  ymax\n",
            "0    raccoon-195.jpg    225     225  raccoon    25   111   197   225\n",
            "1    raccoon-108.jpg    604     481  raccoon    99    53   402   464\n",
            "2    raccoon-119.jpg    400     533  raccoon    16    62   362   353\n",
            "3    raccoon-119.jpg    400     533  raccoon   211   359   277   402\n",
            "4    raccoon-119.jpg    400     533  raccoon   198   392   280   473\n",
            "..               ...    ...     ...      ...   ...   ...   ...   ...\n",
            "212  raccoon-185.jpg    275     183  raccoon    25     1   200   181\n",
            "213   raccoon-31.jpg    236     214  raccoon    82    21   187   197\n",
            "214   raccoon-31.jpg    236     214  raccoon    11    55    80   145\n",
            "215  raccoon-120.jpg    660     371  raccoon   129    12   510   331\n",
            "216   raccoon-68.jpg    640     423  raccoon     1    24   517   423\n",
            "\n",
            "[217 rows x 8 columns]\n",
            "Successfully converted xml to csv.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ioRySI_CwI_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee921164-941b-47fc-a003-004e917d9105"
      },
      "source": [
        "%cd /content\n",
        "!pwd\n",
        "!git clone https://github.com/pytorch/vision.git\n",
        "!cd vision\n",
        "!cp /content/vision/references/detection/utils.py ../\n",
        "!cp /content/vision/references/detection/transforms.py ../\n",
        "!cp /content/vision/references/detection/coco_eval.py ../\n",
        "!cp /content/vision/references/detection/engine.py ../\n",
        "!cp /content/vision/references/detection/coco_utils.py ../\n",
        "%cd ..\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import transforms as T"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content\n",
            "fatal: destination path 'vision' already exists and is not an empty directory.\n",
            "/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrCZQCyTEtSA"
      },
      "source": [
        "def parse_one_annot(path_to_data_file, filename):\n",
        "   data = pd.read_csv(path_to_data_file)\n",
        "   boxes_array = data[data[\"filename\"] == filename][[\"xmin\", \"ymin\",        \n",
        "   \"xmax\", \"ymax\"]].values\n",
        "   return boxes_array\n",
        "\n",
        "class vexaiDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, root, data_file, transforms=None):\n",
        "    self.root = root\n",
        "    self.transforms = transforms\n",
        "    self.imgs = sorted(os.listdir(os.path.join(root, \"images\")))\n",
        "    self.path_to_data_file = data_file\n",
        "  def __getitem__(self, idx):\n",
        "    # load images and bounding boxes\n",
        "    img_path = os.path.join(self.root, \"images\", self.imgs[idx])\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    box_list = parse_one_annot(self.path_to_data_file, self.imgs[idx])\n",
        "    boxes = torch.as_tensor(box_list, dtype=torch.float32)\n",
        "    num_objs = len(box_list)\n",
        "    # there is only one class\n",
        "    labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "    image_id = torch.tensor([idx])\n",
        "    area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:,0])\n",
        "    # suppose all instances are not crowd\n",
        "    iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "    target = {}\n",
        "    target[\"boxes\"] = boxes\n",
        "    target[\"labels\"] = labels\n",
        "    target[\"image_id\"] = image_id\n",
        "    target[\"area\"] = area\n",
        "    target[\"iscrowd\"] = iscrowd\n",
        "    if self.transforms is not None:\n",
        "      img, target = self.transforms(img, target)\n",
        "    return img, target\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_w-BfREE_Hg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52ffd65c-5c10-4703-ec02-9beefb410d57"
      },
      "source": [
        "%cd /content\n",
        "#dataset = vexaiDataset(root = \"/content/Vex-AI-2021-Code/dataset\", data_file = \"/content/Vex-AI-2021-Code/dataset/vexai_labels.csv\")\n",
        "dataset = vexaiDataset(root = \"/content/raccoon_dataset/\", data_file = \"/content/raccoon_dataset/annotations/vexai_labels.csv\")\n",
        "dataset.__getitem__(100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<PIL.Image.Image image mode=RGB size=259x194 at 0x7F8E981C0E10>,\n",
              " {'area': tensor([7695.]),\n",
              "  'boxes': tensor([[ 87.,   8., 182.,  89.]]),\n",
              "  'image_id': tensor([100]),\n",
              "  'iscrowd': tensor([0]),\n",
              "  'labels': tensor([1])})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xApwUST_FPGU"
      },
      "source": [
        "def get_model(num_classes):\n",
        "   # load an object detection model pre-trained on COCO\n",
        "   model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "   # get the number of input features for the classifier\n",
        "   in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "   # replace the pre-trained head with a new on\n",
        "   model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "   \n",
        "   return model\n",
        "\n",
        "def get_transform(train):\n",
        "   transforms = []\n",
        "   # converts the image, a PIL image, into a PyTorch Tensor\n",
        "   transforms.append(T.ToTensor())\n",
        "   if train:\n",
        "      # during training, randomly flip the training images\n",
        "      # and ground-truth for data augmentation\n",
        "      transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "   return T.Compose(transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmgvONg9F5pr",
        "outputId": "fb353243-b5c0-4c46-bcc3-82070482dea7"
      },
      "source": [
        "# use our dataset and defined transformations\n",
        "#dataset = vexaiDataset(root = \"/content/Vex-AI-2021-Code/dataset\", data_file = \"/content/Vex-AI-2021-Code/dataset/vexai_labels.csv\", transforms = get_transform(train=True))\n",
        "#dataset_test = vexaiDataset(root = \"/content/Vex-AI-2021-Code/dataset\", data_file = \"/content/Vex-AI-2021-Code/dataset/vexai_labels.csv\", transforms = get_transform(train=False))\n",
        "dataset = vexaiDataset(root = \"/content/raccoon_dataset/\", data_file = \"/content/raccoon_dataset/annotations/vexai_labels.csv\", transforms = get_transform(train=True))\n",
        "dataset_test = vexaiDataset(root = \"/content/raccoon_dataset/\", data_file = \"/content/raccoon_dataset/annotations/vexai_labels.csv\", transforms = get_transform(train=False))\n",
        "\n",
        "# split the dataset in train and test set\n",
        "torch.manual_seed(1)\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "dataset = torch.utils.data.Subset(dataset, indices[:-40])\n",
        "dataset_test = torch.utils.data.Subset(dataset_test, indices[-40:])\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "              dataset, batch_size=2, shuffle=True, num_workers=4,\n",
        "              collate_fn=utils.collate_fn)\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "         dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
        "         collate_fn=utils.collate_fn)\n",
        "print(\"We have: {} examples, {} are training and {} testing\".format(len(indices), len(dataset), len(dataset_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have: 200 examples, 160 are training and 40 testing\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YVsJYOsGGjB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be66b90e-a297-4b55-93ec-9082999be115"
      },
      "source": [
        "torch.cuda.is_available()\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# our dataset has two classes only - raccoon and not racoon\n",
        "num_classes = 2\n",
        "# get the model using our helper function\n",
        "model = get_model(num_classes)\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005,\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "# and a learning rate scheduler which decreases the learning rate by # 10x every 3 epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                               step_size=3,\n",
        "                                               gamma=0.1)\n",
        "\n",
        "# let's train it for 10 epochs\n",
        "num_epochs = 4\n",
        "for epoch in range(num_epochs):\n",
        "   # train for one epoch, printing every 10 iterations\n",
        "   train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "    # update the learning rate\n",
        "   lr_scheduler.step()\n",
        "   # evaluate on the test dataset\n",
        "   evaluate(model, data_loader_test, device=device)\n",
        "   \n",
        "model_save_name = 'vexaiModel.pt'\n",
        "path = F\"/content/gdrive/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)\n",
        "#torch.save(model.state_dict(), \"model\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [0]  [ 0/80]  eta: 1:11:00  lr: 0.000068  loss: 1.1606 (1.1606)  loss_classifier: 0.9948 (0.9948)  loss_box_reg: 0.1548 (0.1548)  loss_objectness: 0.0006 (0.0006)  loss_rpn_box_reg: 0.0103 (0.0103)  time: 53.2545  data: 0.8169\n",
            "Epoch: [0]  [10/80]  eta: 0:59:45  lr: 0.000701  loss: 0.6500 (0.6856)  loss_classifier: 0.5141 (0.5376)  loss_box_reg: 0.1255 (0.1382)  loss_objectness: 0.0022 (0.0035)  loss_rpn_box_reg: 0.0040 (0.0063)  time: 51.2191  data: 0.0776\n",
            "Epoch: [0]  [20/80]  eta: 0:48:41  lr: 0.001333  loss: 0.3632 (0.4980)  loss_classifier: 0.2009 (0.3517)  loss_box_reg: 0.1151 (0.1350)  loss_objectness: 0.0022 (0.0031)  loss_rpn_box_reg: 0.0040 (0.0082)  time: 48.4671  data: 0.0057\n",
            "Epoch: [0]  [30/80]  eta: 0:40:45  lr: 0.001965  loss: 0.2379 (0.4158)  loss_classifier: 0.1103 (0.2708)  loss_box_reg: 0.1091 (0.1306)  loss_objectness: 0.0022 (0.0040)  loss_rpn_box_reg: 0.0100 (0.0103)  time: 47.6396  data: 0.0088\n",
            "Epoch: [0]  [40/80]  eta: 0:32:14  lr: 0.002597  loss: 0.2054 (0.3614)  loss_classifier: 0.0548 (0.2180)  loss_box_reg: 0.1208 (0.1283)  loss_objectness: 0.0040 (0.0051)  loss_rpn_box_reg: 0.0094 (0.0100)  time: 47.9948  data: 0.0090\n",
            "Epoch: [0]  [50/80]  eta: 0:24:26  lr: 0.003230  loss: 0.1683 (0.3229)  loss_classifier: 0.0444 (0.1827)  loss_box_reg: 0.0954 (0.1227)  loss_objectness: 0.0057 (0.0050)  loss_rpn_box_reg: 0.0094 (0.0125)  time: 48.8646  data: 0.0075\n",
            "Epoch: [0]  [60/80]  eta: 0:16:13  lr: 0.003862  loss: 0.1439 (0.2977)  loss_classifier: 0.0314 (0.1594)  loss_box_reg: 0.0880 (0.1207)  loss_objectness: 0.0017 (0.0046)  loss_rpn_box_reg: 0.0150 (0.0131)  time: 49.4031  data: 0.0072\n",
            "Epoch: [0]  [70/80]  eta: 0:08:07  lr: 0.004494  loss: 0.1592 (0.2925)  loss_classifier: 0.0415 (0.1479)  loss_box_reg: 0.0963 (0.1233)  loss_objectness: 0.0016 (0.0068)  loss_rpn_box_reg: 0.0128 (0.0146)  time: 48.5033  data: 0.0075\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFHHNSrDmUjp"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKC3fOqYmX6X"
      },
      "source": [
        "loaded_model = get_model(num_classes = 2)\n",
        "\n",
        "loaded_model.load_state_dict(torch.load('models'))\n",
        "\n",
        "idx = 0\n",
        "\n",
        "img, _ = dataset_test[idx]\n",
        "\n",
        "label_boxes = np.array(dataset[idx][1][\"boxes\"])\n",
        "#put the model in evaluation mode\n",
        "\n",
        "loaded_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "   prediction = loaded_model([img])\n",
        "\n",
        "image = Image.fromarray(img.mul(255).permute(1, 2,0).byte().numpy())\n",
        "\n",
        "draw = ImageDraw.Draw(image)\n",
        "\n",
        "# draw groundtruth\n",
        "#for elem in range(len(label_boxes)):\n",
        "   \n",
        "#   draw.rectangle([(label_boxes[elem][0], label_boxes[elem][1]), (label_boxes[elem][2], label_boxes[elem][3])], outline =\"green\", width =3)\n",
        "for element in range(len(prediction[0][\"boxes\"])):\n",
        "   boxes = prediction[0][\"boxes\"][element].cpu().numpy()\n",
        "   print(boxes)\n",
        "   score = np.round(prediction[0][\"scores\"][element].cpu().numpy(), decimals= 4)\n",
        "   draw.rectangle([(1,3), (3, 4)], outline =\"red\", width = 3)\n",
        "   if score > 0:\n",
        "      draw.rectangle([(boxes[0], boxes[1]), (boxes[2], boxes[3])],outline =\"red\", width = 3)\n",
        "      draw.text((boxes[0], boxes[1]), text = str(score))\n",
        "\n",
        "image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDmmC6UgMTfj"
      },
      "source": [
        "#append this into developer console to stop automatic timeout:\n",
        "#  https://stackoverflow.com/questions/54057011/google-colab-session-timeout"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}